//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: UNKNOWN
// Unknown Toolkit Version
// Based on LLVM 3.4svn
//

.version 6.3
.target sm_61, texmode_independent
.address_size 64

	// .globl	kernel_vblake
.const .align 1 .b8 sigma[256] = {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 14, 10, 4, 8, 9, 15, 13, 6, 1, 12, 0, 2, 11, 7, 5, 3, 11, 8, 12, 0, 5, 2, 15, 13, 10, 14, 3, 6, 7, 1, 9, 4, 7, 9, 3, 1, 13, 12, 11, 14, 2, 6, 5, 10, 4, 0, 15, 8, 9, 0, 5, 7, 2, 4, 10, 15, 14, 1, 11, 12, 6, 8, 3, 13, 2, 12, 6, 10, 0, 11, 8, 3, 4, 13, 7, 5, 15, 14, 1, 9, 12, 5, 1, 15, 14, 13, 4, 10, 0, 7, 6, 3, 9, 2, 8, 11, 13, 11, 7, 14, 12, 1, 3, 9, 5, 0, 15, 4, 8, 6, 2, 10, 6, 15, 14, 9, 11, 3, 0, 8, 12, 2, 13, 7, 1, 4, 10, 5, 10, 2, 8, 4, 7, 6, 1, 5, 15, 11, 9, 14, 3, 12, 13, 0, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 14, 10, 4, 8, 9, 15, 13, 6, 1, 12, 0, 2, 11, 7, 5, 3, 11, 8, 12, 0, 5, 2, 15, 13, 10, 14, 3, 6, 7, 1, 9, 4, 7, 9, 3, 1, 13, 12, 11, 14, 2, 6, 5, 10, 4, 0, 15, 8, 9, 0, 5, 7, 2, 4, 10, 15, 14, 1, 11, 12, 6, 8, 3, 13, 2, 12, 6, 10, 0, 11, 8, 3, 4, 13, 7, 5, 15, 14, 1, 9};
.const .align 8 .b8 u512[128] = {0, 232, 137, 212, 137, 106, 27, 165, 0, 56, 114, 11, 14, 46, 91, 211, 0, 144, 159, 174, 162, 57, 123, 164, 136, 100, 126, 231, 51, 250, 14, 12, 235, 17, 153, 48, 236, 47, 69, 79, 44, 2, 225, 116, 111, 198, 252, 60, 221, 121, 200, 77, 54, 173, 6, 70, 0, 200, 71, 61, 181, 85, 160, 187, 27, 235, 89, 12, 217, 85, 22, 83, 0, 184, 229, 218, 166, 11, 160, 209, 62, 70, 50, 150, 218, 82, 228, 47, 0, 248, 38, 98, 73, 181, 167, 152, 0, 160, 44, 249, 4, 208, 252, 186, 231, 37, 149, 131, 87, 153, 163, 100, 0, 224, 170, 129, 240, 230, 89, 216, 107, 14, 86, 123, 89, 128, 217, 99};

.entry kernel_vblake(
	.param .u64 .ptr .global .align 4 kernel_vblake_param_0,
	.param .u64 .ptr .global .align 4 kernel_vblake_param_1,
	.param .u64 .ptr .global .align 8 kernel_vblake_param_2,
	.param .u64 .ptr .global .align 8 kernel_vblake_param_3
)
.reqntid 256, 1, 1
{
	.local .align 16 .b8 	__local_depot0[128];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<4>;
	.reg .b32 	%r<159>;
	.reg .b64 	%rd<489>;


	mov.u64 	%SPL, __local_depot0;
	ld.param.u64 	%rd56, [kernel_vblake_param_0];
	ld.param.u64 	%rd57, [kernel_vblake_param_3];
	mov.u32 	%r5, %ctaid.x;
	mov.u32 	%r6, %ntid.x;
	mov.b32	%r7, %envreg3;
	mad.lo.s32 	%r8, %r5, %r6, %r7;
	mov.u32 	%r9, %tid.x;
	add.s32 	%r10, %r8, %r9;
	ld.global.u32 	%r11, [%rd56];
	add.s32 	%r1, %r10, %r11;
	ld.global.u32 	%rd58, [%rd57+56];
	cvt.u64.u32	%rd59, %r1;
	add.u64 	%rd1, %SPL, 0;
	ld.global.u64 	%rd61, [%rd57+8];
	ld.global.u64 	%rd62, [%rd57];
	ld.global.u64 	%rd63, [%rd57+24];
	ld.global.u64 	%rd64, [%rd57+16];
	ld.global.u64 	%rd65, [%rd57+40];
	ld.global.u64 	%rd66, [%rd57+32];
	ld.global.u64 	%rd67, [%rd57+48];
	st.local.v2.u64 	[%rd1], {%rd62, %rd61};
	st.local.v2.u64 	[%rd1+16], {%rd64, %rd63};
	st.local.v2.u64 	[%rd1+32], {%rd66, %rd65};
	bfi.b64 	%rd68, %rd59, %rd58, 32, 32;
	st.local.v2.u64 	[%rd1+48], {%rd67, %rd68};
	mov.u64 	%rd488, 5458154674136264069;
	mov.u64 	%rd487, -5150342838838813098;
	mov.u64 	%rd486, -5150342838838813162;
	mov.u64 	%rd485, 5458154674119421341;
	mov.u64 	%rd481, 6706326878651593006;
	mov.u64 	%rd482, 1363844385690304929;
	mov.u64 	%rd477, -6464152495656057262;
	mov.u64 	%rd479, -5631123829701952856;
	mov.u64 	%rd478, 5631123829701952855;
	mov.u64 	%rd473, -4146243563885628845;
	mov.u64 	%rd474, 7863536789621132781;
	mov.u64 	%rd472, sigma;
	mov.u32 	%r158, -16;
	mov.u64 	%rd475, %rd474;
	mov.u64 	%rd476, %rd473;
	mov.u64 	%rd480, %rd477;
	mov.u64 	%rd483, %rd482;
	mov.u64 	%rd484, %rd481;

BB0_1:
	ld.const.u8 	%r12, [%rd472+1];
	mul.wide.u32 	%rd69, %r12, 8;
	add.s64 	%rd70, %rd1, %rd69;
	mov.u64 	%rd71, u512;
	add.s64 	%rd72, %rd71, %rd69;
	ld.const.u64 	%rd73, [%rd72];
	ld.local.u64 	%rd74, [%rd70];
	xor.b64  	%rd75, %rd73, %rd74;
	add.s64 	%rd76, %rd487, %rd488;
	add.s64 	%rd77, %rd76, %rd75;
	xor.b64  	%rd78, %rd77, %rd486;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r13}, %rd78;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r14,%dummy}, %rd78;
	}
	shf.l.wrap.b32 	%r15, %r14, %r13, 4;
	shf.l.wrap.b32 	%r16, %r13, %r14, 4;
	mov.b64 	%rd79, {%r16, %r15};
	add.s64 	%rd80, %rd485, %rd79;
	xor.b64  	%rd81, %rd487, %rd80;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r17}, %rd81;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r18,%dummy}, %rd81;
	}
	shf.l.wrap.b32 	%r19, %r18, %r17, 21;
	shf.l.wrap.b32 	%r20, %r17, %r18, 21;
	mov.b64 	%rd82, {%r20, %r19};
	add.s64 	%rd83, %rd77, %rd82;
	ld.const.u8 	%r21, [%rd472];
	mul.wide.u32 	%rd84, %r21, 8;
	add.s64 	%rd85, %rd1, %rd84;
	add.s64 	%rd86, %rd71, %rd84;
	ld.const.u64 	%rd87, [%rd86];
	ld.local.u64 	%rd88, [%rd85];
	xor.b64  	%rd89, %rd87, %rd88;
	add.s64 	%rd90, %rd83, %rd89;
	xor.b64  	%rd91, %rd90, %rd79;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r22,%dummy}, %rd91;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r23}, %rd91;
	}
	shf.r.wrap.b32 	%r24, %r23, %r22, 5;
	shf.r.wrap.b32 	%r25, %r22, %r23, 5;
	mov.b64 	%rd92, {%r25, %r24};
	add.s64 	%rd93, %rd80, %rd92;
	xor.b64  	%rd94, %rd82, %rd93;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r26,%dummy}, %rd94;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r27}, %rd94;
	}
	shf.r.wrap.b32 	%r28, %r27, %r26, 18;
	shf.r.wrap.b32 	%r29, %r26, %r27, 18;
	mov.b64 	%rd95, {%r29, %r28};
	not.b64 	%rd96, %rd90;
	not.b64 	%rd97, %rd95;
	and.b64  	%rd98, %rd96, %rd97;
	not.b64 	%rd99, %rd93;
	and.b64  	%rd100, %rd98, %rd99;
	and.b64  	%rd101, %rd95, %rd96;
	and.b64  	%rd102, %rd101, %rd93;
	or.b64  	%rd103, %rd102, %rd100;
	and.b64  	%rd104, %rd93, %rd97;
	and.b64  	%rd105, %rd104, %rd90;
	or.b64  	%rd106, %rd103, %rd105;
	and.b64  	%rd107, %rd95, %rd90;
	and.b64  	%rd108, %rd107, %rd99;
	or.b64  	%rd109, %rd106, %rd108;
	xor.b64  	%rd110, %rd109, %rd92;
	and.b64  	%rd111, %rd98, %rd93;
	and.b64  	%rd112, %rd96, %rd99;
	and.b64  	%rd113, %rd112, %rd95;
	or.b64  	%rd114, %rd113, %rd111;
	and.b64  	%rd115, %rd90, %rd97;
	and.b64  	%rd116, %rd115, %rd99;
	or.b64  	%rd117, %rd114, %rd116;
	and.b64  	%rd118, %rd107, %rd93;
	or.b64  	%rd119, %rd117, %rd118;
	xor.b64  	%rd120, %rd119, %rd110;
	ld.const.u8 	%r30, [%rd472+3];
	mul.wide.u32 	%rd121, %r30, 8;
	add.s64 	%rd122, %rd1, %rd121;
	add.s64 	%rd123, %rd71, %rd121;
	ld.const.u64 	%rd124, [%rd123];
	ld.local.u64 	%rd125, [%rd122];
	xor.b64  	%rd126, %rd124, %rd125;
	add.s64 	%rd127, %rd483, %rd484;
	add.s64 	%rd128, %rd127, %rd126;
	xor.b64  	%rd129, %rd128, %rd482;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r31}, %rd129;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r32,%dummy}, %rd129;
	}
	shf.l.wrap.b32 	%r33, %r32, %r31, 4;
	shf.l.wrap.b32 	%r34, %r31, %r32, 4;
	mov.b64 	%rd130, {%r34, %r33};
	add.s64 	%rd131, %rd481, %rd130;
	xor.b64  	%rd132, %rd483, %rd131;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r35}, %rd132;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r36,%dummy}, %rd132;
	}
	shf.l.wrap.b32 	%r37, %r36, %r35, 21;
	shf.l.wrap.b32 	%r38, %r35, %r36, 21;
	mov.b64 	%rd133, {%r38, %r37};
	add.s64 	%rd134, %rd128, %rd133;
	ld.const.u8 	%r39, [%rd472+2];
	mul.wide.u32 	%rd135, %r39, 8;
	add.s64 	%rd136, %rd1, %rd135;
	add.s64 	%rd137, %rd71, %rd135;
	ld.const.u64 	%rd138, [%rd137];
	ld.local.u64 	%rd139, [%rd136];
	xor.b64  	%rd140, %rd138, %rd139;
	add.s64 	%rd141, %rd134, %rd140;
	xor.b64  	%rd142, %rd141, %rd130;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r40,%dummy}, %rd142;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r41}, %rd142;
	}
	shf.r.wrap.b32 	%r42, %r41, %r40, 5;
	shf.r.wrap.b32 	%r43, %r40, %r41, 5;
	mov.b64 	%rd143, {%r43, %r42};
	add.s64 	%rd144, %rd131, %rd143;
	xor.b64  	%rd145, %rd133, %rd144;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r44,%dummy}, %rd145;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r45}, %rd145;
	}
	shf.r.wrap.b32 	%r46, %r45, %r44, 18;
	shf.r.wrap.b32 	%r47, %r44, %r45, 18;
	mov.b64 	%rd146, {%r47, %r46};
	not.b64 	%rd147, %rd141;
	not.b64 	%rd148, %rd146;
	and.b64  	%rd149, %rd147, %rd148;
	not.b64 	%rd150, %rd144;
	and.b64  	%rd151, %rd149, %rd150;
	and.b64  	%rd152, %rd146, %rd147;
	and.b64  	%rd153, %rd152, %rd144;
	or.b64  	%rd154, %rd153, %rd151;
	and.b64  	%rd155, %rd144, %rd148;
	and.b64  	%rd156, %rd155, %rd141;
	or.b64  	%rd157, %rd154, %rd156;
	and.b64  	%rd158, %rd146, %rd141;
	and.b64  	%rd159, %rd158, %rd150;
	or.b64  	%rd160, %rd157, %rd159;
	xor.b64  	%rd161, %rd160, %rd143;
	and.b64  	%rd162, %rd149, %rd144;
	and.b64  	%rd163, %rd147, %rd150;
	and.b64  	%rd164, %rd163, %rd146;
	or.b64  	%rd165, %rd164, %rd162;
	and.b64  	%rd166, %rd141, %rd148;
	and.b64  	%rd167, %rd166, %rd150;
	or.b64  	%rd168, %rd165, %rd167;
	and.b64  	%rd169, %rd158, %rd144;
	or.b64  	%rd170, %rd168, %rd169;
	xor.b64  	%rd171, %rd170, %rd161;
	ld.const.u8 	%r48, [%rd472+5];
	mul.wide.u32 	%rd172, %r48, 8;
	add.s64 	%rd173, %rd1, %rd172;
	add.s64 	%rd174, %rd71, %rd172;
	ld.const.u64 	%rd175, [%rd174];
	ld.local.u64 	%rd176, [%rd173];
	xor.b64  	%rd177, %rd175, %rd176;
	add.s64 	%rd178, %rd479, %rd480;
	add.s64 	%rd179, %rd178, %rd177;
	xor.b64  	%rd180, %rd179, %rd478;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r49}, %rd180;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r50,%dummy}, %rd180;
	}
	shf.l.wrap.b32 	%r51, %r50, %r49, 4;
	shf.l.wrap.b32 	%r52, %r49, %r50, 4;
	mov.b64 	%rd181, {%r52, %r51};
	add.s64 	%rd182, %rd477, %rd181;
	xor.b64  	%rd183, %rd479, %rd182;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r53}, %rd183;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r54,%dummy}, %rd183;
	}
	shf.l.wrap.b32 	%r55, %r54, %r53, 21;
	shf.l.wrap.b32 	%r56, %r53, %r54, 21;
	mov.b64 	%rd184, {%r56, %r55};
	add.s64 	%rd185, %rd179, %rd184;
	ld.const.u8 	%r57, [%rd472+4];
	mul.wide.u32 	%rd186, %r57, 8;
	add.s64 	%rd187, %rd1, %rd186;
	add.s64 	%rd188, %rd71, %rd186;
	ld.const.u64 	%rd189, [%rd188];
	ld.local.u64 	%rd190, [%rd187];
	xor.b64  	%rd191, %rd189, %rd190;
	add.s64 	%rd192, %rd185, %rd191;
	xor.b64  	%rd193, %rd192, %rd181;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r58,%dummy}, %rd193;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r59}, %rd193;
	}
	shf.r.wrap.b32 	%r60, %r59, %r58, 5;
	shf.r.wrap.b32 	%r61, %r58, %r59, 5;
	mov.b64 	%rd194, {%r61, %r60};
	add.s64 	%rd195, %rd182, %rd194;
	xor.b64  	%rd196, %rd184, %rd195;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r62,%dummy}, %rd196;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r63}, %rd196;
	}
	shf.r.wrap.b32 	%r64, %r63, %r62, 18;
	shf.r.wrap.b32 	%r65, %r62, %r63, 18;
	mov.b64 	%rd197, {%r65, %r64};
	not.b64 	%rd198, %rd192;
	not.b64 	%rd199, %rd197;
	and.b64  	%rd200, %rd198, %rd199;
	not.b64 	%rd201, %rd195;
	and.b64  	%rd202, %rd200, %rd201;
	and.b64  	%rd203, %rd197, %rd198;
	and.b64  	%rd204, %rd203, %rd195;
	or.b64  	%rd205, %rd204, %rd202;
	and.b64  	%rd206, %rd195, %rd199;
	and.b64  	%rd207, %rd206, %rd192;
	or.b64  	%rd208, %rd205, %rd207;
	and.b64  	%rd209, %rd197, %rd192;
	and.b64  	%rd210, %rd209, %rd201;
	or.b64  	%rd211, %rd208, %rd210;
	xor.b64  	%rd212, %rd211, %rd194;
	and.b64  	%rd213, %rd200, %rd195;
	and.b64  	%rd214, %rd198, %rd201;
	and.b64  	%rd215, %rd214, %rd197;
	or.b64  	%rd216, %rd215, %rd213;
	and.b64  	%rd217, %rd192, %rd199;
	and.b64  	%rd218, %rd217, %rd201;
	or.b64  	%rd219, %rd216, %rd218;
	and.b64  	%rd220, %rd209, %rd195;
	or.b64  	%rd221, %rd219, %rd220;
	xor.b64  	%rd222, %rd221, %rd212;
	ld.const.u8 	%r66, [%rd472+7];
	mul.wide.u32 	%rd223, %r66, 8;
	add.s64 	%rd224, %rd1, %rd223;
	add.s64 	%rd225, %rd71, %rd223;
	ld.const.u64 	%rd226, [%rd225];
	ld.local.u64 	%rd227, [%rd224];
	xor.b64  	%rd228, %rd226, %rd227;
	add.s64 	%rd229, %rd475, %rd476;
	add.s64 	%rd230, %rd229, %rd228;
	xor.b64  	%rd231, %rd230, %rd474;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r67}, %rd231;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r68,%dummy}, %rd231;
	}
	shf.l.wrap.b32 	%r69, %r68, %r67, 4;
	shf.l.wrap.b32 	%r70, %r67, %r68, 4;
	mov.b64 	%rd232, {%r70, %r69};
	add.s64 	%rd233, %rd473, %rd232;
	xor.b64  	%rd234, %rd475, %rd233;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r71}, %rd234;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r72,%dummy}, %rd234;
	}
	shf.l.wrap.b32 	%r73, %r72, %r71, 21;
	shf.l.wrap.b32 	%r74, %r71, %r72, 21;
	mov.b64 	%rd235, {%r74, %r73};
	add.s64 	%rd236, %rd230, %rd235;
	ld.const.u8 	%r75, [%rd472+6];
	mul.wide.u32 	%rd237, %r75, 8;
	add.s64 	%rd238, %rd1, %rd237;
	add.s64 	%rd239, %rd71, %rd237;
	ld.const.u64 	%rd240, [%rd239];
	ld.local.u64 	%rd241, [%rd238];
	xor.b64  	%rd242, %rd240, %rd241;
	add.s64 	%rd243, %rd236, %rd242;
	xor.b64  	%rd244, %rd243, %rd232;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r76,%dummy}, %rd244;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r77}, %rd244;
	}
	shf.r.wrap.b32 	%r78, %r77, %r76, 5;
	shf.r.wrap.b32 	%r79, %r76, %r77, 5;
	mov.b64 	%rd245, {%r79, %r78};
	add.s64 	%rd246, %rd233, %rd245;
	xor.b64  	%rd247, %rd235, %rd246;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r80,%dummy}, %rd247;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r81}, %rd247;
	}
	shf.r.wrap.b32 	%r82, %r81, %r80, 18;
	shf.r.wrap.b32 	%r83, %r80, %r81, 18;
	mov.b64 	%rd248, {%r83, %r82};
	not.b64 	%rd249, %rd243;
	not.b64 	%rd250, %rd248;
	and.b64  	%rd251, %rd249, %rd250;
	not.b64 	%rd252, %rd246;
	and.b64  	%rd253, %rd251, %rd252;
	and.b64  	%rd254, %rd248, %rd249;
	and.b64  	%rd255, %rd254, %rd246;
	or.b64  	%rd256, %rd255, %rd253;
	and.b64  	%rd257, %rd246, %rd250;
	and.b64  	%rd258, %rd257, %rd243;
	or.b64  	%rd259, %rd256, %rd258;
	and.b64  	%rd260, %rd248, %rd243;
	and.b64  	%rd261, %rd260, %rd252;
	or.b64  	%rd262, %rd259, %rd261;
	xor.b64  	%rd263, %rd262, %rd245;
	and.b64  	%rd264, %rd251, %rd246;
	and.b64  	%rd265, %rd249, %rd252;
	and.b64  	%rd266, %rd265, %rd248;
	or.b64  	%rd267, %rd266, %rd264;
	and.b64  	%rd268, %rd243, %rd250;
	and.b64  	%rd269, %rd268, %rd252;
	or.b64  	%rd270, %rd267, %rd269;
	and.b64  	%rd271, %rd260, %rd246;
	or.b64  	%rd272, %rd270, %rd271;
	xor.b64  	%rd273, %rd272, %rd263;
	add.s64 	%rd274, %rd146, %rd90;
	ld.const.u8 	%r84, [%rd472+9];
	mul.wide.u32 	%rd275, %r84, 8;
	add.s64 	%rd276, %rd1, %rd275;
	add.s64 	%rd277, %rd71, %rd275;
	ld.const.u64 	%rd278, [%rd277];
	ld.local.u64 	%rd279, [%rd276];
	xor.b64  	%rd280, %rd278, %rd279;
	add.s64 	%rd281, %rd274, %rd280;
	xor.b64  	%rd282, %rd281, %rd273;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r85}, %rd282;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r86,%dummy}, %rd282;
	}
	shf.l.wrap.b32 	%r87, %r86, %r85, 4;
	shf.l.wrap.b32 	%r88, %r85, %r86, 4;
	mov.b64 	%rd283, {%r88, %r87};
	add.s64 	%rd284, %rd195, %rd283;
	xor.b64  	%rd285, %rd146, %rd284;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r89}, %rd285;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r90,%dummy}, %rd285;
	}
	shf.l.wrap.b32 	%r91, %r90, %r89, 21;
	shf.l.wrap.b32 	%r92, %r89, %r90, 21;
	mov.b64 	%rd286, {%r92, %r91};
	add.s64 	%rd287, %rd281, %rd286;
	ld.const.u8 	%r93, [%rd472+8];
	mul.wide.u32 	%rd288, %r93, 8;
	add.s64 	%rd289, %rd1, %rd288;
	add.s64 	%rd290, %rd71, %rd288;
	ld.const.u64 	%rd291, [%rd290];
	ld.local.u64 	%rd292, [%rd289];
	xor.b64  	%rd293, %rd291, %rd292;
	add.s64 	%rd488, %rd287, %rd293;
	xor.b64  	%rd294, %rd488, %rd283;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r94,%dummy}, %rd294;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r95}, %rd294;
	}
	shf.r.wrap.b32 	%r96, %r95, %r94, 5;
	shf.r.wrap.b32 	%r97, %r94, %r95, 5;
	mov.b64 	%rd295, {%r97, %r96};
	add.s64 	%rd477, %rd284, %rd295;
	xor.b64  	%rd296, %rd286, %rd477;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r98,%dummy}, %rd296;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r99}, %rd296;
	}
	shf.r.wrap.b32 	%r100, %r99, %r98, 18;
	shf.r.wrap.b32 	%r101, %r98, %r99, 18;
	mov.b64 	%rd483, {%r101, %r100};
	not.b64 	%rd297, %rd488;
	not.b64 	%rd298, %rd483;
	and.b64  	%rd299, %rd297, %rd298;
	not.b64 	%rd300, %rd477;
	and.b64  	%rd301, %rd299, %rd300;
	and.b64  	%rd302, %rd483, %rd297;
	and.b64  	%rd303, %rd302, %rd477;
	or.b64  	%rd304, %rd303, %rd301;
	and.b64  	%rd305, %rd477, %rd298;
	and.b64  	%rd306, %rd305, %rd488;
	or.b64  	%rd307, %rd304, %rd306;
	and.b64  	%rd308, %rd483, %rd488;
	and.b64  	%rd309, %rd308, %rd300;
	or.b64  	%rd310, %rd307, %rd309;
	xor.b64  	%rd311, %rd310, %rd295;
	and.b64  	%rd312, %rd299, %rd477;
	and.b64  	%rd313, %rd297, %rd300;
	and.b64  	%rd314, %rd313, %rd483;
	or.b64  	%rd315, %rd314, %rd312;
	and.b64  	%rd316, %rd488, %rd298;
	and.b64  	%rd317, %rd316, %rd300;
	or.b64  	%rd318, %rd315, %rd317;
	and.b64  	%rd319, %rd308, %rd477;
	or.b64  	%rd320, %rd318, %rd319;
	xor.b64  	%rd474, %rd320, %rd311;
	add.s64 	%rd321, %rd197, %rd141;
	ld.const.u8 	%r102, [%rd472+11];
	mul.wide.u32 	%rd322, %r102, 8;
	add.s64 	%rd323, %rd1, %rd322;
	add.s64 	%rd324, %rd71, %rd322;
	ld.const.u64 	%rd325, [%rd324];
	ld.local.u64 	%rd326, [%rd323];
	xor.b64  	%rd327, %rd325, %rd326;
	add.s64 	%rd328, %rd321, %rd327;
	xor.b64  	%rd329, %rd328, %rd120;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r103}, %rd329;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r104,%dummy}, %rd329;
	}
	shf.l.wrap.b32 	%r105, %r104, %r103, 4;
	shf.l.wrap.b32 	%r106, %r103, %r104, 4;
	mov.b64 	%rd330, {%r106, %r105};
	add.s64 	%rd331, %rd246, %rd330;
	xor.b64  	%rd332, %rd197, %rd331;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r107}, %rd332;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r108,%dummy}, %rd332;
	}
	shf.l.wrap.b32 	%r109, %r108, %r107, 21;
	shf.l.wrap.b32 	%r110, %r107, %r108, 21;
	mov.b64 	%rd333, {%r110, %r109};
	add.s64 	%rd334, %rd328, %rd333;
	ld.const.u8 	%r111, [%rd472+10];
	mul.wide.u32 	%rd335, %r111, 8;
	add.s64 	%rd336, %rd1, %rd335;
	add.s64 	%rd337, %rd71, %rd335;
	ld.const.u64 	%rd338, [%rd337];
	ld.local.u64 	%rd339, [%rd336];
	xor.b64  	%rd340, %rd338, %rd339;
	add.s64 	%rd484, %rd334, %rd340;
	xor.b64  	%rd341, %rd484, %rd330;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r112,%dummy}, %rd341;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r113}, %rd341;
	}
	shf.r.wrap.b32 	%r114, %r113, %r112, 5;
	shf.r.wrap.b32 	%r115, %r112, %r113, 5;
	mov.b64 	%rd342, {%r115, %r114};
	add.s64 	%rd473, %rd331, %rd342;
	xor.b64  	%rd343, %rd333, %rd473;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r116,%dummy}, %rd343;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r117}, %rd343;
	}
	shf.r.wrap.b32 	%r118, %r117, %r116, 18;
	shf.r.wrap.b32 	%r119, %r116, %r117, 18;
	mov.b64 	%rd479, {%r119, %r118};
	not.b64 	%rd344, %rd484;
	not.b64 	%rd345, %rd479;
	and.b64  	%rd346, %rd344, %rd345;
	not.b64 	%rd347, %rd473;
	and.b64  	%rd348, %rd346, %rd347;
	and.b64  	%rd349, %rd479, %rd344;
	and.b64  	%rd350, %rd349, %rd473;
	or.b64  	%rd351, %rd350, %rd348;
	and.b64  	%rd352, %rd473, %rd345;
	and.b64  	%rd353, %rd352, %rd484;
	or.b64  	%rd354, %rd351, %rd353;
	and.b64  	%rd355, %rd479, %rd484;
	and.b64  	%rd356, %rd355, %rd347;
	or.b64  	%rd357, %rd354, %rd356;
	xor.b64  	%rd358, %rd357, %rd342;
	and.b64  	%rd359, %rd346, %rd473;
	and.b64  	%rd360, %rd344, %rd347;
	and.b64  	%rd361, %rd360, %rd479;
	or.b64  	%rd362, %rd361, %rd359;
	and.b64  	%rd363, %rd484, %rd345;
	and.b64  	%rd364, %rd363, %rd347;
	or.b64  	%rd365, %rd362, %rd364;
	and.b64  	%rd366, %rd355, %rd473;
	or.b64  	%rd367, %rd365, %rd366;
	xor.b64  	%rd486, %rd367, %rd358;
	add.s64 	%rd368, %rd248, %rd192;
	ld.const.u8 	%r120, [%rd472+13];
	mul.wide.u32 	%rd369, %r120, 8;
	add.s64 	%rd370, %rd1, %rd369;
	add.s64 	%rd371, %rd71, %rd369;
	ld.const.u64 	%rd372, [%rd371];
	ld.local.u64 	%rd373, [%rd370];
	xor.b64  	%rd374, %rd372, %rd373;
	add.s64 	%rd375, %rd368, %rd374;
	xor.b64  	%rd376, %rd375, %rd171;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r121}, %rd376;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r122,%dummy}, %rd376;
	}
	shf.l.wrap.b32 	%r123, %r122, %r121, 4;
	shf.l.wrap.b32 	%r124, %r121, %r122, 4;
	mov.b64 	%rd377, {%r124, %r123};
	add.s64 	%rd378, %rd93, %rd377;
	xor.b64  	%rd379, %rd248, %rd378;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r125}, %rd379;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r126,%dummy}, %rd379;
	}
	shf.l.wrap.b32 	%r127, %r126, %r125, 21;
	shf.l.wrap.b32 	%r128, %r125, %r126, 21;
	mov.b64 	%rd380, {%r128, %r127};
	add.s64 	%rd381, %rd375, %rd380;
	ld.const.u8 	%r129, [%rd472+12];
	mul.wide.u32 	%rd382, %r129, 8;
	add.s64 	%rd383, %rd1, %rd382;
	add.s64 	%rd384, %rd71, %rd382;
	ld.const.u64 	%rd385, [%rd384];
	ld.local.u64 	%rd386, [%rd383];
	xor.b64  	%rd387, %rd385, %rd386;
	add.s64 	%rd480, %rd381, %rd387;
	xor.b64  	%rd388, %rd480, %rd377;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r130,%dummy}, %rd388;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r131}, %rd388;
	}
	shf.r.wrap.b32 	%r132, %r131, %r130, 5;
	shf.r.wrap.b32 	%r133, %r130, %r131, 5;
	mov.b64 	%rd389, {%r133, %r132};
	add.s64 	%rd485, %rd378, %rd389;
	xor.b64  	%rd390, %rd380, %rd485;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r134,%dummy}, %rd390;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r135}, %rd390;
	}
	shf.r.wrap.b32 	%r136, %r135, %r134, 18;
	shf.r.wrap.b32 	%r137, %r134, %r135, 18;
	mov.b64 	%rd475, {%r137, %r136};
	not.b64 	%rd391, %rd480;
	not.b64 	%rd392, %rd475;
	and.b64  	%rd393, %rd391, %rd392;
	not.b64 	%rd394, %rd485;
	and.b64  	%rd395, %rd393, %rd394;
	and.b64  	%rd396, %rd475, %rd391;
	and.b64  	%rd397, %rd396, %rd485;
	or.b64  	%rd398, %rd397, %rd395;
	and.b64  	%rd399, %rd485, %rd392;
	and.b64  	%rd400, %rd399, %rd480;
	or.b64  	%rd401, %rd398, %rd400;
	and.b64  	%rd402, %rd475, %rd480;
	and.b64  	%rd403, %rd402, %rd394;
	or.b64  	%rd404, %rd401, %rd403;
	xor.b64  	%rd405, %rd404, %rd389;
	and.b64  	%rd406, %rd393, %rd485;
	and.b64  	%rd407, %rd391, %rd394;
	and.b64  	%rd408, %rd407, %rd475;
	or.b64  	%rd409, %rd408, %rd406;
	and.b64  	%rd410, %rd480, %rd392;
	and.b64  	%rd411, %rd410, %rd394;
	or.b64  	%rd412, %rd409, %rd411;
	and.b64  	%rd413, %rd402, %rd485;
	or.b64  	%rd414, %rd412, %rd413;
	xor.b64  	%rd482, %rd414, %rd405;
	add.s64 	%rd415, %rd95, %rd243;
	ld.const.u8 	%r138, [%rd472+15];
	mul.wide.u32 	%rd416, %r138, 8;
	add.s64 	%rd417, %rd1, %rd416;
	add.s64 	%rd418, %rd71, %rd416;
	ld.const.u64 	%rd419, [%rd418];
	ld.local.u64 	%rd420, [%rd417];
	xor.b64  	%rd421, %rd419, %rd420;
	add.s64 	%rd422, %rd415, %rd421;
	xor.b64  	%rd423, %rd422, %rd222;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r139}, %rd423;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r140,%dummy}, %rd423;
	}
	shf.l.wrap.b32 	%r141, %r140, %r139, 4;
	shf.l.wrap.b32 	%r142, %r139, %r140, 4;
	mov.b64 	%rd424, {%r142, %r141};
	add.s64 	%rd425, %rd144, %rd424;
	xor.b64  	%rd426, %rd95, %rd425;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r143}, %rd426;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r144,%dummy}, %rd426;
	}
	shf.l.wrap.b32 	%r145, %r144, %r143, 21;
	shf.l.wrap.b32 	%r146, %r143, %r144, 21;
	mov.b64 	%rd427, {%r146, %r145};
	add.s64 	%rd428, %rd422, %rd427;
	ld.const.u8 	%r147, [%rd472+14];
	mul.wide.u32 	%rd429, %r147, 8;
	add.s64 	%rd430, %rd1, %rd429;
	add.s64 	%rd431, %rd71, %rd429;
	ld.const.u64 	%rd432, [%rd431];
	ld.local.u64 	%rd433, [%rd430];
	xor.b64  	%rd434, %rd432, %rd433;
	add.s64 	%rd476, %rd428, %rd434;
	xor.b64  	%rd435, %rd476, %rd424;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r148,%dummy}, %rd435;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r149}, %rd435;
	}
	shf.r.wrap.b32 	%r150, %r149, %r148, 5;
	shf.r.wrap.b32 	%r151, %r148, %r149, 5;
	mov.b64 	%rd436, {%r151, %r150};
	add.s64 	%rd481, %rd425, %rd436;
	xor.b64  	%rd437, %rd427, %rd481;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r152,%dummy}, %rd437;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r153}, %rd437;
	}
	shf.r.wrap.b32 	%r154, %r153, %r152, 18;
	shf.r.wrap.b32 	%r155, %r152, %r153, 18;
	mov.b64 	%rd487, {%r155, %r154};
	not.b64 	%rd438, %rd476;
	not.b64 	%rd439, %rd487;
	and.b64  	%rd440, %rd438, %rd439;
	not.b64 	%rd441, %rd481;
	and.b64  	%rd442, %rd440, %rd441;
	and.b64  	%rd443, %rd487, %rd438;
	and.b64  	%rd444, %rd443, %rd481;
	or.b64  	%rd445, %rd444, %rd442;
	and.b64  	%rd446, %rd481, %rd439;
	and.b64  	%rd447, %rd446, %rd476;
	or.b64  	%rd448, %rd445, %rd447;
	and.b64  	%rd449, %rd487, %rd476;
	and.b64  	%rd450, %rd449, %rd441;
	or.b64  	%rd451, %rd448, %rd450;
	xor.b64  	%rd452, %rd451, %rd436;
	and.b64  	%rd453, %rd440, %rd481;
	and.b64  	%rd454, %rd438, %rd441;
	and.b64  	%rd455, %rd454, %rd487;
	or.b64  	%rd456, %rd455, %rd453;
	and.b64  	%rd457, %rd476, %rd439;
	and.b64  	%rd458, %rd457, %rd441;
	or.b64  	%rd459, %rd456, %rd458;
	and.b64  	%rd460, %rd449, %rd481;
	or.b64  	%rd461, %rd459, %rd460;
	xor.b64  	%rd478, %rd461, %rd452;
	add.s64 	%rd472, %rd472, 16;
	add.s32 	%r158, %r158, 1;
	setp.ne.s32	%p1, %r158, 0;
	@%p1 bra 	BB0_1;

	xor.b64  	%rd462, %rd478, %rd479;
	xor.b64  	%rd463, %rd473, %rd476;
	xor.b64  	%rd464, %rd463, %rd462;
	xor.b64  	%rd465, %rd485, %rd488;
	xor.b64  	%rd466, %rd464, %rd465;
	xor.b64  	%rd36, %rd466, 4328219849969886590;
	cvt.u32.u64	%r156, %rd466;
	xor.b32  	%r157, %r156, -1958746754;
	setp.ne.s32	%p2, %r157, 0;
	@%p2 bra 	BB0_5;

	ld.param.u64 	%rd470, [kernel_vblake_param_2];
	ld.global.u64 	%rd467, [%rd470];
	add.s64 	%rd468, %rd467, -1;
	setp.lt.u64	%p3, %rd468, %rd36;
	@%p3 bra 	BB0_5;

	ld.param.u64 	%rd471, [kernel_vblake_param_2];
	ld.param.u64 	%rd469, [kernel_vblake_param_1];
	st.global.u32 	[%rd469], %r1;
	st.global.u64 	[%rd471], %rd36;

BB0_5:
	ret;
}


.metadata_section {

.metadata 0 {
	"cl_kernel_attributes",
	"kernel_vblake",
	"reqd_work_group_size(256,1,1)"
}

} // end of .metadata_section
  